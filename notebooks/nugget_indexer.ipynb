{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import mkdir, path\n",
    "from subprocess import Popen, PIPE\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TREC_TEXT_TEMPLATE = (\n",
    "    '<DOC>\\n'\n",
    "    '<DOCNO>{}</DOCNO>\\n'\n",
    "    '<TEXT>\\n'\n",
    "    '{}\\n'\n",
    "    '</TEXT>\\n'\n",
    "    '</DOC>\\n')\n",
    "\n",
    "\n",
    "def build_trec_text(serial, text):\n",
    "    return TREC_TEXT_TEMPLATE.format(serial, text)\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    with Popen(['java', 'edu.stanford.nlp.process.PTBTokenizer', '-preserveLines'],\n",
    "               stdin=PIPE, stdout=PIPE, stderr=PIPE) as tokenizer_proc:\n",
    "        out, err = tokenizer_proc.communicate(input=text.encode('UTF-8'))\n",
    "        return out.decode('UTF-8')\n",
    "\n",
    "\n",
    "def tokenize_ja(text):\n",
    "    return ' '.join(list(text.strip()))\n",
    "\n",
    "\n",
    "def write_files(doc_set):\n",
    "    for serial, doc in doc_set.items():\n",
    "        with open(doc['filepath'], 'w') as f:\n",
    "            f.write(doc['trec_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nugget_dict(root, topic_id_set):\n",
    "    nugget_dict = {}\n",
    "    for topic_id in topic_id_set:\n",
    "        exam = root.find('exam/[@id=\"{}\"]'.format(topic_id))\n",
    "        if not exam:\n",
    "            continue\n",
    "        for ans in exam.iter('answer'):\n",
    "            annotator_id = ans.get('annotator')\n",
    "            for sem in ans.iter('semantic_unit'):\n",
    "                sem_id = sem.get('id')\n",
    "                for prop in sem.iter('proposition'):\n",
    "                    prop_id = prop.get('id')\n",
    "                    nugget_id = '{}_{}_{}_{}'.format(topic_id, annotator_id, sem_id, prop_id)\n",
    "                    nugget_text = prop.get('value').strip()\n",
    "                    nugget_dict[nugget_id] = nugget_text\n",
    "    return nugget_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_folder_path_en = '../../qalab3-essay-phase2/_references/qalab3-en-essay-phase2/qalab3-en-phase2-nugget-essay'\n",
    "\n",
    "nugget_dict_en = dict()\n",
    "topic_id_set_en = {'B792W10_[1]', 'C792W10_[1]'}\n",
    "for topic_id in topic_id_set_en:\n",
    "    tree_en = et.parse('{}/{}'.format(ref_folder_path_en, '{}.xml'.format(topic_id.replace('_[1]', ''))))\n",
    "    root_en = tree_en.getroot()\n",
    "    nugget_dict_en.update(get_nugget_dict(root_en, topic_id_set_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_OUTPUT_DIR = '../../qalab3-essay-phase2/nuggets-en-tokenized/'\n",
    "if not path.isdir(TOKENIZED_OUTPUT_DIR):\n",
    "    mkdir(TOKENIZED_OUTPUT_DIR)\n",
    "\n",
    "tokenized_docs_en = {}\n",
    "total = len(nugget_dict_en)\n",
    "count = 0\n",
    "print('tokenizing {} docs...'.format(total))\n",
    "for serial, doc in nugget_dict_en.items():\n",
    "    tokenized_text = tokenize_en(doc)\n",
    "    tokenized_trec_text = build_trec_text(serial, tokenized_text)\n",
    "    filepath = path.join(TOKENIZED_OUTPUT_DIR, '{}-seg.xml'.format(serial))\n",
    "    tokenized_docs_en[serial] = {'trec_text': tokenized_trec_text, 'filepath': filepath}\n",
    "    count += 1\n",
    "    if count % 10 == 0 or count == total:\n",
    "        print('tokenized {}/{} docs'.format(count, total), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_files(tokenized_docs_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_folder_path_ja = '../../qalab3-essay-phase2/_references/qalab3-ja-essay-phase2/qalab3-ja-phase2-nugget-essay'\n",
    "\n",
    "nugget_dict_ja = dict()\n",
    "topic_id_set_ja = {'B792W10_【１】', 'C792W10_【１】'}\n",
    "for topic_id in topic_id_set_ja:\n",
    "    tree_ja = et.parse('{}/{}'.format(ref_folder_path_ja, '{}.xml'.format(topic_id.replace('_【１】', ''))))\n",
    "    root_ja = tree_ja.getroot()\n",
    "    nugget_dict_ja.update(get_nugget_dict(root_ja, topic_id_set_ja))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZED_OUTPUT_DIR = '../../qalab3-essay-phase2/nuggets-ja-tokenized/'\n",
    "if not path.isdir(TOKENIZED_OUTPUT_DIR):\n",
    "    mkdir(TOKENIZED_OUTPUT_DIR)\n",
    "\n",
    "tokenized_docs_ja = {}\n",
    "total = len(nugget_dict_ja)\n",
    "count = 0\n",
    "print('tokenizing {} docs...'.format(total))\n",
    "for serial, doc in nugget_dict_ja.items():\n",
    "    tokenized_text = tokenize_ja(doc)\n",
    "    tokenized_trec_text = build_trec_text(serial, tokenized_text)\n",
    "    filepath = path.join(TOKENIZED_OUTPUT_DIR, '{}-seg.xml'.format(serial))\n",
    "    tokenized_docs_ja[serial] = {'trec_text': tokenized_trec_text, 'filepath': filepath}\n",
    "    count += 1\n",
    "    if count % 10 == 0 or count == total:\n",
    "        print('tokenized {}/{} docs'.format(count, total), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_files(tokenized_docs_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
