{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as et\n",
    "\n",
    "\n",
    "def get_element_texts(parent_element, xpath):\n",
    "    texts = []\n",
    "    for element in parent_element.findall(xpath):\n",
    "        text = ''.join(element.itertext())\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_expressions_dict(root):\n",
    "    expressions_dict = {}\n",
    "    for answer_section in root.iter('answer_section'):\n",
    "        section_id = answer_section.get('id')\n",
    "        expression_set = answer_section.findall('answer_set/answer/expression_set/expression')\n",
    "        expressions = [''.join(expression.itertext()) for expression in expression_set]\n",
    "        expressions_dict[section_id] = expressions\n",
    "    return expressions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "\n",
    "mecab = MeCab.Tagger('-Owakati')\n",
    "\n",
    "\n",
    "def get_tokenized_line_ja(line):\n",
    "    return mecab.parse(line).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    with Popen(['java', 'edu.stanford.nlp.process.PTBTokenizer', '-preserveLines'],\n",
    "               stdin=PIPE, stdout=PIPE, stderr=PIPE) as tokenizer_proc:\n",
    "        out, err = tokenizer_proc.communicate(input=text.encode('UTF-8'))\n",
    "        return out.decode('UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rank_score_ja(eval_tree, gold_dict, model_ja, name_prefix):\n",
    "    for topic in eval_tree.getroot().iter('TOPIC'):\n",
    "        topic_id = topic.get('ID')\n",
    "        ans_dict = {}\n",
    "        for ans in topic.findall('ANSWER_SET/ANSWER'):\n",
    "            ans_file = ans.get('FILE_NAME')\n",
    "            ans_text = ''.join(ans.itertext())\n",
    "            ans_dict[ans_file] = ans_text\n",
    "        score_dict = {}\n",
    "        tokenized_golds_words = []\n",
    "        golds = gold_dict[topic_id]\n",
    "        for gold in golds:\n",
    "            tokenized_gold = get_tokenized_line_ja(gold)\n",
    "            tokenized_golds_words.append(tokenized_gold.split())\n",
    "        score = 0\n",
    "        for name, text in ans_dict.items():\n",
    "            tokenized_text_words = get_tokenized_line_ja(text).split()\n",
    "            for tokenized_gold_words in tokenized_golds_words:\n",
    "                score += model_ja.wmdistance(tokenized_text_words, tokenized_gold_words)\n",
    "            score /= 3\n",
    "            score_dict[name] = score\n",
    "        for rank, name in enumerate(sorted(score_dict, key=score_dict.get)):\n",
    "            score = score_dict[name]\n",
    "            ans = topic.find('ANSWER_SET/ANSWER/[@FILE_NAME=\"{}\"]'.format(name))\n",
    "            ans.set('RANK', str(rank + 1))\n",
    "            ans.set('SCORE', str(score))\n",
    "    eval_tree.write('{}_DGLab_01.xml'.format(name_prefix), encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_rank_score_en(eval_tree, gold_dict, model_en, name_prefix):\n",
    "    for topic in eval_tree.getroot().iter('TOPIC'):\n",
    "        topic_id = topic.get('ID')\n",
    "        ans_dict = {}\n",
    "        for ans in topic.findall('ANSWER_SET/ANSWER'):\n",
    "            ans_file = ans.get('FILE_NAME')\n",
    "            ans_text = ''.join(ans.itertext())\n",
    "            ans_dict[ans_file] = ans_text\n",
    "        score_dict = {}\n",
    "        tokenized_golds_words = []\n",
    "        golds = gold_dict[topic_id]\n",
    "        for tokenized_gold in golds:\n",
    "            tokenized_golds_words.append(tokenized_gold.split())\n",
    "        score = 0\n",
    "        for name, text in ans_dict.items():\n",
    "            tokenized_text_words = tokenize_en(text).split()\n",
    "            for tokenized_gold_words in tokenized_golds_words:\n",
    "                score += model_en.wmdistance(tokenized_text_words, tokenized_gold_words)\n",
    "            score /= 3\n",
    "            score_dict[name] = score\n",
    "        for rank, name in enumerate(sorted(score_dict, key=score_dict.get)):\n",
    "            score = score_dict[name]\n",
    "            ans = topic.find('ANSWER_SET/ANSWER/[@FILE_NAME=\"{}\"]'.format(name))\n",
    "            ans.set('RANK', str(rank + 1))\n",
    "            ans.set('SCORE', str(score))\n",
    "    eval_tree.write('{}_DGLab_01.xml'.format(name_prefix), encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "\n",
    "model_ja = Word2Vec.load('w2v_ja.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_ja = et.parse('qalab3-essay-phase2/_references/qalab3-ja-essay-phase2/qalab3-ja-phase2-goldstandard-essay.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dict_ja = get_expressions_dict(tree_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ja = et.parse('qalab3-essay-phase2/_references/qalab3-ja-essay-phase2/qalab3-ja-phase2-essay-evaluationmethod.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rank_score_ja(eval_ja, gold_dict_ja, model_ja, 'qalab3-ja-phase2-essay-evaluationmethod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_en = Word2Vec.load('w2v_en.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_en = et.parse('qalab3-essay-phase2/_references/qalab3-en-essay-phase2/qalab3-en-phase2-goldstandard-essay.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_dict_en = get_expressions_dict(tree_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_en = et.parse('qalab3-essay-phase2/_references/qalab3-en-essay-phase2/qalab3-en-phase2-essay-evaluationmethod.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_rank_score_en(eval_en, gold_dict_en, model_en, 'qalab3-en-phase2-essay-evaluationmethod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
