{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import mkdir, path\n",
    "from subprocess import Popen, PIPE\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TREC_TEXT_TEMPLATE = (\n",
    "    '<DOC>\\n'\n",
    "    '<DOCNO>{}</DOCNO>\\n'\n",
    "    '<TEXT>\\n'\n",
    "    '{}\\n'\n",
    "    '</TEXT>\\n'\n",
    "    '</DOC>\\n')\n",
    "\n",
    "\n",
    "def build_trec_text(serial, text):\n",
    "    return TREC_TEXT_TEMPLATE.format(serial, text)\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    with Popen(['java', 'edu.stanford.nlp.process.PTBTokenizer', '-preserveLines'],\n",
    "               stdin=PIPE, stdout=PIPE, stderr=PIPE) as tokenizer_proc:\n",
    "        out, err = tokenizer_proc.communicate(input=text.encode('UTF-8'))\n",
    "        return out.decode('UTF-8')\n",
    "\n",
    "\n",
    "def tokenize_ja(text):\n",
    "    return ' '.join(list(text.strip()))\n",
    "\n",
    "\n",
    "def write_files(doc_set):\n",
    "    for serial, doc in doc_set.items():\n",
    "        with open(doc['filepath'], 'w') as f:\n",
    "            f.write(doc['trec_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_passage_dict(root, topic_id_set):\n",
    "    passage_dict = {}\n",
    "    for topic_id in topic_id_set:\n",
    "        topic = root.find('TOPIC/[@ID=\"{}\"]'.format(topic_id))\n",
    "        for passage_set in topic.iter('PASSAGE_SET'):\n",
    "            passage_set_name = passage_set.get('FILE_NAME')\n",
    "            for passage in passage_set.iter('PASSAGE'):\n",
    "                passage_id = '{}_{}_{}'.format(topic_id, passage_set_name, passage.get('RANK'))\n",
    "                passage_text = ''.join(passage.itertext()).strip()\n",
    "                passage_dict[passage_id] = passage_text\n",
    "    return passage_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_folder_path_en = '../../qalab3-essay-phase2/_references/qalab3-en-essay-phase2'\n",
    "passage_file_name_en = 'qalab3-en-phase2-essay-extraction-for-evaluation.xml'\n",
    "\n",
    "tree_en = et.parse('{}/{}'.format(ref_folder_path_en, passage_file_name_en))\n",
    "root_en = tree_en.getroot()\n",
    "topic_id_set_en = {'B792W10-1', 'C792W10-1'}\n",
    "passage_dict_en = get_passage_dict(root_en, topic_id_set_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing 477 docs...\n",
      "tokenized 100/477 docs\r"
     ]
    }
   ],
   "source": [
    "TOKENIZED_OUTPUT_DIR = '../../qalab3-essay-phase2/passages-en-tokenized/'\n",
    "if not path.isdir(TOKENIZED_OUTPUT_DIR):\n",
    "    mkdir(TOKENIZED_OUTPUT_DIR)\n",
    "\n",
    "tokenized_docs_en = {}\n",
    "total = len(passage_dict_en)\n",
    "count = 0\n",
    "print('tokenizing {} docs...'.format(total))\n",
    "for serial, doc in passage_dict_en.items():\n",
    "    tokenized_text = tokenize_en(doc)\n",
    "    tokenized_trec_text = build_trec_text(serial, tokenized_text)\n",
    "    filepath = path.join(TOKENIZED_OUTPUT_DIR, '{}-seg.xml'.format(serial))\n",
    "    tokenized_docs_en[serial] = {'trec_text': tokenized_trec_text, 'filepath': filepath}\n",
    "    count += 1\n",
    "    if count % 100 == 0 or count == total:\n",
    "        print('tokenized {}/{} docs'.format(count, total), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_files(tokenized_docs_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_folder_path_ja = '../../qalab3-essay-phase2/_references/qalab3-ja-essay-phase2'\n",
    "passage_file_name_ja = 'qalab3-ja-phase2-essay-extraction-for-evaluation.xml'\n",
    "\n",
    "tree_ja = et.parse('{}/{}'.format(ref_folder_path_ja, passage_file_name_ja))\n",
    "root_ja = tree_ja.getroot()\n",
    "topic_id_set_ja = {'B792W10-1', 'C792W10-1'}\n",
    "passage_dict_ja = get_passage_dict(root_ja, topic_id_set_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing 692 docs...\n",
      "tokenized 100/692 docs\r",
      "tokenized 200/692 docs\r",
      "tokenized 300/692 docs\r",
      "tokenized 400/692 docs\r",
      "tokenized 500/692 docs\r",
      "tokenized 600/692 docs\r",
      "tokenized 692/692 docs\r"
     ]
    }
   ],
   "source": [
    "TOKENIZED_OUTPUT_DIR = '../../qalab3-essay-phase2/passages-ja-tokenized/'\n",
    "if not path.isdir(TOKENIZED_OUTPUT_DIR):\n",
    "    mkdir(TOKENIZED_OUTPUT_DIR)\n",
    "\n",
    "tokenized_docs_ja = {}\n",
    "total = len(passage_dict_ja)\n",
    "count = 0\n",
    "print('tokenizing {} docs...'.format(total))\n",
    "for serial, doc in passage_dict_ja.items():\n",
    "    tokenized_text = tokenize_ja(doc)\n",
    "    tokenized_trec_text = build_trec_text(serial, tokenized_text)\n",
    "    filepath = path.join(TOKENIZED_OUTPUT_DIR, '{}-seg.xml'.format(serial))\n",
    "    tokenized_docs_ja[serial] = {'trec_text': tokenized_trec_text, 'filepath': filepath}\n",
    "    count += 1\n",
    "    if count % 100 == 0 or count == total:\n",
    "        print('tokenized {}/{} docs'.format(count, total), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_files(tokenized_docs_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
