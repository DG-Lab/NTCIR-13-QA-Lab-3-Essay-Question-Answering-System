{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import re\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "\n",
    "def get_length_limit(parent_element):\n",
    "    return int(re.findall(r'\\d+', parent_element.find(\n",
    "        'answer_set/answer/[@length_limit]').get('length_limit'))[0])\n",
    "\n",
    "\n",
    "def _select_passage_sentence(passage_and_src_pair_list, keywords, get_sentence_func):\n",
    "    keyword_pattern = '|'.join(map(lambda x: '({})'.format(x), keywords))\n",
    "    keyword_regex = re.compile(keyword_pattern)\n",
    "    matched_sentence_list = []\n",
    "    sentence_dict = {}\n",
    "    for passage, src in passage_and_src_pair_list:\n",
    "        sentences = get_sentence_func(passage)\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            matches = keyword_regex.findall(sentence)\n",
    "            if matches:\n",
    "                if sentence not in sentence_dict:\n",
    "                    matched_sentence_list.append({\n",
    "                        'src': '{}[{}]'.format(src, i),\n",
    "                        'sentence': sentence,\n",
    "                        'rate': len(matches) / len(sentence)})\n",
    "                    sentence_dict[sentence] = len(matched_sentence_list) - 1\n",
    "                else:\n",
    "                    matched_sentence_list[sentence_dict[sentence]]['src'] += ',{}[{}]'.format(src, i)\n",
    "    sorted(matched_sentence_list, key=lambda x: x['rate'], reverse=True)\n",
    "    return [(entry['sentence'], entry['src']) for entry in matched_sentence_list]\n",
    "\n",
    "\n",
    "sentence_stop_ja = re.compile(r'([。？！])')\n",
    "def _get_sentence_ja(text):\n",
    "    pieces = sentence_stop_ja.split(text)\n",
    "    return [x + y for x, y in zip(pieces[::2], pieces[1::2])]\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "def _get_sentence_en(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_func_truncate_ja(passage_and_src_pair_list, len_limit):\n",
    "    summary_text = None\n",
    "    src_list = []\n",
    "    for passage, src in passage_and_src_pair_list:\n",
    "        if not summary_text:\n",
    "            summary_text = passage[:len_limit]\n",
    "            src_list.append(src)\n",
    "        elif len(summary_text) + len(passage) <= len_limit:\n",
    "            summary_text += '\\n' + passage\n",
    "            src_list.append(src)\n",
    "        else:\n",
    "            break\n",
    "    return summary_text, src_list\n",
    "\n",
    "\n",
    "def summarize_func_truncate_en(passage_and_src_pair_list, len_limit):\n",
    "    summary_tokens = []\n",
    "    src_list = []\n",
    "    current_length = 0\n",
    "    for passage, src in passage_and_src_pair_list:\n",
    "        passage_tokens = passage.split(' ')\n",
    "        if not summary_tokens:\n",
    "            summary_tokens.extend(passage_tokens)\n",
    "            current_length += len(passage_tokens)\n",
    "            src_list.append(src)\n",
    "            if len(summary_tokens) > len_limit:\n",
    "                return ' '.join(summary_tokens[:len_limit]), src_list\n",
    "        elif current_length + len(passage_tokens) <= len_limit:\n",
    "            summary_tokens.extend(passage_tokens)\n",
    "            current_length += len(passage_tokens)\n",
    "            src_list.append(src)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(summary_tokens[:len_limit]), src_list\n",
    "\n",
    "\n",
    "def get_summary_set_by_participant(exp_root, ans_root, summarize_func):\n",
    "    summaries = {}\n",
    "    for topic in exp_root.iter('TOPIC'):\n",
    "        topic_id = topic.get('ID')\n",
    "        ans_section = ans_root_ja.find('answer_section/[@id=\"{}\"]'.format(topic_id))\n",
    "        ans_len_limit = get_length_limit(ans_section)\n",
    "        for passage_set in topic.iter('PASSAGE_SET'):\n",
    "            passage_set_name = passage_set.get('FILE_NAME')\n",
    "            if passage_set_name not in summaries:\n",
    "                summaries[passage_set_name] = {}\n",
    "            \n",
    "            passage_and_src_pairs = []\n",
    "            for passage in passage_set.iter('PASSAGE'):\n",
    "                passage_id = passage.get('SOURCE_ID')\n",
    "                passage_text = ''.join(passage.itertext()).strip()\n",
    "                passage_and_src_pairs.append((passage_text, passage_id))\n",
    "            summary_text, passage_id_list = summarize_func(passage_and_src_pairs, ans_len_limit)\n",
    "            summaries[passage_set_name][topic_id] = (summary_text, passage_id_list)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def write_xml_by_participant(summaries, ans_tree, tgt_folder_path, ans_file_name):\n",
    "    for passage_set_name, sections in summaries.items():\n",
    "        ans_tree_copy = deepcopy(ans_tree)\n",
    "        ans_root_copy = ans_tree_copy.getroot()\n",
    "        for ans_section_id, summary_tuple in sections.items():\n",
    "            summary = summary_tuple[0]\n",
    "            passage_id_str = ','.join(summary_tuple[1]) \n",
    "            ans_root_copy.set('src', passage_set_name)\n",
    "            expression = ans_root_copy.find(\n",
    "                'answer_section/[@id=\"{}\"]/answer_set/answer/expression_set/expression'.format(\n",
    "                    ans_section_id))\n",
    "            expression.text = summary\n",
    "            expression.set('source_id', passage_id_str)\n",
    "        ans_tree_copy.write('{}/{}_DGLab_summarization_ExP10_{}_01.xml'.format(\n",
    "            tgt_folder_path, ans_file_name, passage_set_name\n",
    "        ), encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_run_folder_path = 'qalab3-essay-phase2/'\n",
    "ref_folder_path_ja = 'qalab3-essay-phase2/_references/qalab3-ja-essay-phase2'\n",
    "\n",
    "ans_file_name_ja = 'qalab3-ja-phase2-answersheet-essay' \n",
    "ans_tree_ja = et.parse('{}/{}.xml'.format(ref_folder_path_ja, ans_file_name_ja))\n",
    "ans_root_ja = ans_tree_ja.getroot()\n",
    "\n",
    "passage_file_name_ja = 'qalab3-ja-phase2-essay-extraction-ExP10_revised.xml'\n",
    "exp_tree_ja = et.parse('{}/{}'.format(ref_folder_path_ja, passage_file_name_ja))\n",
    "exp_root_ja = exp_tree_ja.getroot()\n",
    "\n",
    "summary_set_ja = get_summary_set_by_participant(exp_root_ja, ans_root_ja, summarize_func_truncate_ja)\n",
    "write_xml_by_participant(summary_set_ja, ans_tree_ja, temp_run_folder_path, ans_file_name_ja)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_run_folder_path = 'qalab3-essay-phase2/'\n",
    "ref_folder_path_en = 'qalab3-essay-phase2/_references/qalab3-en-essay-phase2'\n",
    "\n",
    "ans_file_name_en = 'qalab3-en-phase2-answersheet-essay' \n",
    "ans_tree_en = et.parse('{}/{}.xml'.format(ref_folder_path_en, ans_file_name_en))\n",
    "ans_root_en = ans_tree_en.getroot()\n",
    "\n",
    "passage_file_name_en = 'qalab3-en-phase2-essay-extraction-ExP10_revised.xml'\n",
    "exp_tree_en = et.parse('{}/{}'.format(ref_folder_path_en, passage_file_name_en))\n",
    "exp_root_en = exp_tree_en.getroot()\n",
    "\n",
    "summary_set_en = get_summary_set_by_participant(exp_root_en, ans_root_en, summarize_func_truncate_en)\n",
    "write_xml_by_participant(summary_set_en, ans_tree_en, temp_run_folder_path, ans_file_name_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def passage_filter_ja(passage_and_src_pair_list, keywords):\n",
    "    return _select_passage_sentence(passage_and_src_pair_list, keywords, _get_sentence_ja)\n",
    "\n",
    "\n",
    "def passage_filter_en(passage_and_src_pair_list, keywords):\n",
    "    return _select_passage_sentence(passage_and_src_pair_list, keywords, _get_sentence_en)\n",
    "\n",
    "\n",
    "def get_element_texts(parent_element, xpath):\n",
    "    texts = []\n",
    "    for element in parent_element.findall(xpath):\n",
    "        text = ''.join(element.itertext())\n",
    "        if text:\n",
    "            texts.append(text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_summary_set_for_all(exp_root, ans_root, summarize_func, passage_filter_func):\n",
    "    summaries = {}\n",
    "    for topic in exp_root.iter('TOPIC'):\n",
    "        topic_id = topic.get('ID')\n",
    "        ans_section = ans_root.find('answer_section/[@id=\"{}\"]'.format(topic_id))\n",
    "        ans_len_limit = get_length_limit(ans_section)\n",
    "        keywords = get_element_texts(ans_section, 'keyword_set/keyword')\n",
    "        passage_and_src_pairs = []\n",
    "        for passage_set in topic.iter('PASSAGE_SET'):\n",
    "            passage_set_name = passage_set.get('FILE_NAME')\n",
    "\n",
    "            for passage in passage_set.iter('PASSAGE'):\n",
    "                passage_id = passage_set_name + '_' + passage.get('SOURCE_ID')\n",
    "                passage_text = ''.join(passage.itertext()).strip()\n",
    "                passage_and_src_pairs.append((passage_text, passage_id))\n",
    "        passage_and_src_pairs = passage_filter_func(passage_and_src_pairs, keywords)\n",
    "        summary_text, passage_id_list = summarize_func(passage_and_src_pairs, ans_len_limit)\n",
    "        summaries[topic_id] = (summary_text, passage_id_list)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def write_xml_by_extracton_file(summaries, ans_tree, tgt_folder_path, ans_file_name, extraction_file_id):\n",
    "    ans_tree_copy = deepcopy(ans_tree)\n",
    "    ans_root_copy = ans_tree_copy.getroot()\n",
    "    for ans_sect in ans_root_copy.findall('answer_section'):\n",
    "        ans_sect_id = ans_sect.get('id')\n",
    "        if ans_sect_id not in summaries:\n",
    "            ans_root_copy.remove(ans_sect)\n",
    "\n",
    "    for ans_section_id, summary_tuple in summaries.items():\n",
    "        summary = summary_tuple[0]\n",
    "        passage_id_str = ','.join(summary_tuple[1]) \n",
    "        ans_root_copy.set('src', extraction_file_id)\n",
    "        expression = ans_root_copy.find(\n",
    "            'answer_section/[@id=\"{}\"]/answer_set/answer/expression_set/expression'.format(ans_section_id))\n",
    "        expression.text = summary\n",
    "        expression.set('source_id', passage_id_str)\n",
    "    ans_tree_copy.write('{}/{}_DGLab_summarization_{}_01.xml'.format(\n",
    "        tgt_folder_path, ans_file_name, extraction_file_id\n",
    "    ), encoding='UTF-8', xml_declaration=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_run_folder_path = 'qalab3-essay-phase2/'\n",
    "ref_folder_path_ja = 'qalab3-essay-phase2/_references/qalab3-ja-essay-phase2'\n",
    "\n",
    "ans_file_name_ja = 'qalab3-ja-phase2-answersheet-essay' \n",
    "ans_tree_ja = et.parse('{}/{}.xml'.format(ref_folder_path_ja, ans_file_name_ja))\n",
    "ans_root_ja = ans_tree_ja.getroot()\n",
    "\n",
    "passage_file_id_list = ['ExP10_revised', 'GSN+ExP10_revised', 'GSN']\n",
    "for passage_file_id in passage_file_id_list:\n",
    "    passage_file_name_ja = 'qalab3-ja-phase2-essay-extraction-{}.xml'.format(passage_file_id)\n",
    "    exp_tree_ja = et.parse('{}/{}'.format(ref_folder_path_ja, passage_file_name_ja))\n",
    "    exp_root_ja = exp_tree_ja.getroot()\n",
    "\n",
    "    summary_set_ja = get_summary_set_for_all(exp_root_ja, ans_root_ja, summarize_func_truncate_ja, passage_filter_ja)\n",
    "    write_xml_by_extracton_file(summary_set_ja, ans_tree_ja, temp_run_folder_path, ans_file_name_ja, passage_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_run_folder_path = 'qalab3-essay-phase2/'\n",
    "ref_folder_path_en = 'qalab3-essay-phase2/_references/qalab3-en-essay-phase2'\n",
    "\n",
    "ans_file_name_en = 'qalab3-en-phase2-answersheet-essay' \n",
    "ans_tree_en = et.parse('{}/{}.xml'.format(ref_folder_path_en, ans_file_name_en))\n",
    "ans_root_en = ans_tree_en.getroot()\n",
    "\n",
    "passage_file_id_list = ['ExP10_revised', 'GSN+ExP10_revised', 'GSN']\n",
    "for passage_file_id in passage_file_id_list:\n",
    "    passage_file_name_en = 'qalab3-en-phase2-essay-extraction-{}.xml'.format(passage_file_id)\n",
    "    exp_tree_en = et.parse('{}/{}'.format(ref_folder_path_en, passage_file_name_en))\n",
    "    exp_root_en = exp_tree_en.getroot()\n",
    "\n",
    "    summary_set_en = get_summary_set_for_all(exp_root_en, ans_root_en, summarize_func_truncate_en, passage_filter_en)\n",
    "    write_xml_by_extracton_file(summary_set_en, ans_tree_en, temp_run_folder_path, ans_file_name_en, passage_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from subprocess import Popen, PIPE, TimeoutExpired\n",
    "\n",
    "\n",
    "def shuca_ja(text, length):\n",
    "    echo = Popen(['echo', text], stdout=PIPE)\n",
    "    juman = Popen(['juman'], stdin=echo.stdout, stdout=PIPE)\n",
    "    echo.stdout.close()\n",
    "    knp = Popen(['knp', '-anaphora', '-case', '-tab'], stdin=juman.stdout, stdout=PIPE, stderr=PIPE)\n",
    "    knp_output, knp_error = knp.communicate()\n",
    "    knp_text = knp_output.decode('UTF-8')\n",
    "    juman.stdout.close()\n",
    "    with open('temp.txt', 'a') as f:\n",
    "        f.write(knp_text)\n",
    "    with open('err.txt', 'a') as f:\n",
    "        f.write(knp_error.decode('UTF-8'))\n",
    "    knp.stdout.close()\n",
    "    knp.terminate()\n",
    "    juman.terminate()\n",
    "    echo.terminate()\n",
    "\n",
    "    shuca = Popen(['shuca/lib/Shuca.py', '-l', str(length)], stdin=PIPE, stdout=PIPE, stderr=PIPE)\n",
    "    output = None\n",
    "    error = None\n",
    "    output, error = shuca.communicate(input=knp_output)\n",
    "    if error:\n",
    "        print('Shuca error: {}'.format(error.decode('UTF-8')))\n",
    "    if output:\n",
    "        summary = output.decode('UTF-8')\n",
    "    else:\n",
    "        summary = ''\n",
    "    shuca.stdout.close()\n",
    "    shuca.terminate()\n",
    "\n",
    "    summary = '\\n'.join([line.strip() for line in summary.splitlines() if line.strip()])\n",
    "    print('Shuca output: {}'.format(summary))\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summarize_func_shuca_ja(passage_and_src_pair_list, len_limit):\n",
    "    print('\\noriginal sentence count={}'.format(len(passage_and_src_pair_list)))\n",
    "    passage_and_src_pair_list = passage_and_src_pair_list[:25]\n",
    "    passages, src_list = zip(*passage_and_src_pair_list)\n",
    "    # passages = list(map(lambda x: re.sub(r'\\s', '', x), passages))\n",
    "    passage_text = '\\n'.join(passages)\n",
    "    summary_text = shuca_ja(passage_text, len_limit)\n",
    "    return summary_text, src_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original sentence count=206\n",
      "Shuca output: オスマン帝国（オスマントルコ語：دولتعليهعثمانیه、Devlet-i'Alīye-i'Osmānīye、現代トルコ語：OsmanlıİmparatorluğuまたはOsmanlıDevleti）は、トルコ帝国、オスマントルコとしても知られています。\n",
      "また、タハリール広場から東に400mほどのところにムハンマド・アリー朝の王宮であり、現大統領府であるアブディーン宮殿がある。\n",
      "タハリール広場から南のナイル川沿いはガーデン・シティと呼ばれ、イギリス統治時代にエジプト総督府がおかれ開発が進められたエリアである。\n",
      "とくに19世紀後半のエジプト太守イスマーイール・パシャは近代化に熱心であり、スエズ運河の開通にあわせてナイル川東岸の低湿地を開発して、パリの都市計画に倣った新市街を旧市街の西側に建設した。\n",
      "やがて西方を拠点としたオクタヴィアヌスと東方に拠るアントニウスの対立がおこり，オクタヴィアヌスは，プトレマイオス朝エジプトのクレオパトラと結んだアントニウスを前３１年アクティウムの海戦でやぶった。\n",
      "しかし，アントニウスがプトレマイオス朝エジプトの女王クレオパトラと結んだため，オクタヴィアヌスはこの連合軍を，前３１年，アクティウムの海戦でやぶった。\n",
      "\n",
      "original sentence count=173\n",
      "Shuca output: １４世紀後半，倭寇の活動と，交易の自由化がもたらす国内経済の動揺に悩まされた明の洪武帝は，国内経済の復活と治安の安定のために，中国商人の対外交易と渡航を禁止した（海禁）。\n",
      "また16世紀後半、中国の海禁政策がゆるむと、多数の中国商人が来航するようになり、東南アジアの海の交易は活況を呈した。\n",
      "また中国の対外貿易は、あくまでも中華帝国側が恩恵的に許可する朝貢貿易であり、明初には倭寇対策もあって、民間人の海洋貿易や海上交通が法令として禁止(海禁)された。\n",
      "16世紀にはいり、それまで海禁政策のもとにあった中国商人がさかんに密貿易をおこなうようになると、琉球王国による中継貿易は急速におとろえ、日本との貿易も堺や博多などの商人にとってかわられた。\n",
      "16世紀に入ると、中国人や日本人、朝鮮人の私貿易商人たちは武装して海禁に抵抗し、中国や朝鮮の沿岸都市を襲った。\n",
      "孫文の中国は、辛亥革命のあとも軍閥割拠により、長い内戦状態を経験せねばならなかった。\n",
      "\n",
      "original sentence count=217\n",
      "Shuca output: また、1920年1月に国際連盟が設立された。\n",
      "またそれまでナショナリズムによって統合されていた国民国家の中でも民族や部族などの単位に分離するエスノ・ナショナリズムが掲げられるようにもなっている。\n",
      "１８世紀後半の啓蒙専制体制以来オーストリアでは，徴兵制にもとづく軍制改革や農奴解放などの社会改革がすすめられていたが，ナショナリズムが台頭する時代に入って，領域内の多数の民族をまとめるうえでの困難が大きくなっていた。\n",
      "ナショナリズムは、ナショナリズムは、通常、日本語では以下のような三つの訳語があてられている。\n",
      "第三に、ナショナリズムは19世紀末になると、排外主義的傾向を強め、国家主義の意味をもつようになる。\n",
      "ふたたび戦争をくり返してはならないという願いをこめて、1920年、ジュネーヴに本部をおく国際連盟が成立した。\n",
      "調査団は軍事行動が自衛権の発動であるとする日本の主張をしりぞけ、連盟もそれを支持したので、日本は33(昭和8)年3月、国際連盟脱退を通告した。\n",
      "戦争の長期化にともない、前例のない物量戦・消耗戦となり、総力戦というこの戦争の性格があらわになってきた。\n",
      "\n",
      "original sentence count=262\n",
      "Shuca output: その後、カール5世によって、ネーデルラント17州がハプスブルク家の支配下に統合される。\n",
      "ニューヨークやボストンの港は奴隷貿易港として栄えるようになった。\n",
      "南アフリカ戦争、ブール戦争ともいう。\n",
      "ヨーロッパ統合は経済・市場統合にとどまらず，さらに共通の外交・防衛政策を採用して政治統合をめざすヨーロッパ連合条約（マーストリヒト条約）が１９９３年に発効され，ヨーロッパ連合（ＥＵ）が成立した。\n",
      "１９９２年，ＥＣ憲法ともいわれるローマ条約を改定してあらたにマーストリヒト条約（欧州連合条約）を締結し，９３年に発効するとＥＵ（欧州連合）と改称した。\n",
      "太平洋戦争開始とともに，ドイツ・イタリアもアメリカ合衆国に宣戦し，日本・ドイツ・イタリアらの枢軸国（ファシズム陣営）と，アメリカ・イギリス・ソ連ら連合国（反ファシズム陣営）の戦争となり，文字どおりの世界大戦になった。\n",
      "日米交渉はゆきづまり，１９４１年１２月８日，日本はハワイの真珠湾を奇襲し，アメリカとイギリスに宣戦して太平洋戦争（アジア太平洋戦争）がはじまった。\n",
      "しかし，太平洋戦争（アジア太平洋戦争）がはじまると全土が日本軍の占領下におかれ，独立の達成には戦争の終結をまたなければならなかった。\n",
      "世界的規模で展開されるハンバーガーショップ、コーヒーショップで、人々はいまや同じ材料、同じ製法でつくられた食物、飲み物を、世界の各都市で消費するようになった。\n",
      "\n",
      "original sentence count=258\n",
      "Shuca output: ロシアは，カフカスでは１９世紀初頭にグルジアを直轄植民地とし，さらにアルメニアの帰属をめぐって，イランのカージャール朝と二度にわたって戦い，１８２８年のトルコマンチャーイ条約でアルメニアとアゼルバイジャン北半を植民地とした。\n",
      "西アジア方面ではカージャール朝ペルシアに対してトルコマンチャーイ条約を受け入れさせ、アルメニアを併合することに成功した。\n",
      "エジプト事件に際してはオスマン帝国を終始支援し、ウンキャル・スケレッシ条約を締結してボスポラス・ダーダネルス両海峡の独占航行権を一時獲得するなど、南下政策を進めていったが、イギリスやプロイセン・オーストリアの干渉を受けて挫折し、オスマン帝国との間にクリミア戦争を起こした。\n",
      "カージャール朝は，１９世紀前半にカフカースの領有をめぐるロシアとの戦争にやぶれて不平等条約（トルコマンチャーイ条約）をおしつけられ，またアフガニスタンをめぐるイギリスとの戦争にもやぶれて，以後，ヨーロッパ諸国に経済的，軍事的に従属するようになった。\n",
      "初代の東シベリア総督ムラヴィヨフはアムール川（黒龍江）地方を占領し，１８５８年には，清が太平天国やアロー戦争による苦境にあったのに乗じ，清に強要してアムール川以北の地を獲得し，ウスリー川以東の沿海州を両国の共同管理とした（アイグン条約）。\n",
      "\n",
      "original sentence count=251\n",
      "Shuca output: 古代エジプトは、ナイル川を中心に、古王国から新王国まで、長らく独立王朝が栄えた。\n",
      "クレオパトラは、ローマの内乱に際し、アクティウムの海戦に敗れた。\n",
      "アクティウムの海戦の後、エジプトはローマの属州となった。\n",
      "イスラム教勢力は紀元7世紀にアラビア半島を統一した。\n",
      "イスラム教勢力は東ローマ帝国とササン朝ペルシアの対立に乗じた。\n",
      "イスラム教勢力は版図を拡大した。\n",
      "イスラム教勢力はエジプトを征服した。\n",
      "ファーティマ朝では、1196年にサラディンが宰相となった。\n",
      "十字軍はイスラム教勢力から聖地エルサレムを奪還しようとした。\n",
      "サラディンは十字軍に対抗した。\n",
      "近代には、フランスのナポレオンがエジプトに進攻した。\n",
      "ナポレオンがエジプトに進攻したのは、イギリス本国とインドとの連絡線を断つためだった。\n",
      "ナポレオンは、エジプトで解放者を称した。\n",
      "ナポレオンは、エジプトで民衆の抵抗に遭った。\n",
      "対仏戦争に際して、オスマン帝国は、ムハンマド・アリーをエジプトに派遣した。\n",
      "ムハンマド・アリーは、エジプトに王朝を建てた。\n",
      "ムハンマド・アリー朝はイギリスによる内政への介入を受けた。\n",
      "ムハンマド・アリー朝はイギリスの保護国となった。\n",
      "エジプトはナイル川流域に穀倉地帯を形成した。\n",
      "7世紀以降、イスラム教徒はアフリカにまで勢力を拡大した。\n",
      "\n",
      "original sentence count=204\n",
      "Shuca output: アヘン戦争後、中国から、多くの移民が苦力として移住した。\n",
      "華僑は、マレー半島におけるイギリスの海峡植民地のサトウキビ・プランテーションで働いた。\n",
      "華僑は、利権回収運動を資金的に援助した。\n",
      "利権回収運動は、鉄道敷設権や鉱山採決権などの利権の回収を目指した。\n",
      "華僑は、孫文の革命運動を資金的に援助した。\n",
      "孫文の革命運動は清朝を打倒しようとした。\n",
      "明代以降、海禁政策が行われた。\n",
      "アヘン戦争での敗北で、海禁政策は廃止された。\n",
      "海禁政策廃止後、中国では海外渡航の自由化が承認された。\n",
      "アヘン戦争後、中国人労働者の生活は困窮していた。\n",
      "イギリスは海峡植民地を建設した。\n",
      "イギリスは海峡植民地を拠点に英領マラヤを建国した。\n",
      "イギリスは海峡植民地を中心に、ゴム・プランテーションを展開した。\n",
      "清は1840年のアヘン戦争の敗北により開国した。\n",
      "西部開拓の進展や南北戦争後にリンカーンが行った1863年の奴隷解放宣言、イギリスによる1833年の植民地奴隷制の廃止で、海峡植民地や南米のサトウキビ・プランテーションに於ける労働者の不足が深刻化した。\n",
      "\n",
      "original sentence count=258\n",
      "Shuca output: 30年戦争は、ウェストファリア条約によって終止符が打たれた。\n",
      "ウェストファリア条約は多くの勢力が参加して調印された。\n",
      "ウェストファリア条約によって、主権国家体制が成立した。\n",
      "グロティウスは、『戦争と平和の法』を著した。\n",
      "グロティウスは、『戦争と平和の法』で戦時において各国の守るべき権利・義務を述べた。\n",
      "諸外国に対して、徴兵制による国民軍が用いられた。\n",
      "第一次世界大戦では、ウィルソンによる十四カ条の平和原則が見られた。\n",
      "第一次世界大戦では、ソ連が平和に関する布告を発表した。\n",
      "第一次世界大戦後には、国際連盟が設立された。\n",
      "国際連盟は、集団安全保障の枠組みである。\n",
      "三十年戦争の講和条約はウェストファリア条約であった。\n",
      "ウェストファリア条約によってヨーロッパの主権国家体制が確立された。\n",
      "三十年戦争の惨禍を受けて、グロティウスは『戦争と平和の法』を著した。\n",
      "グロティウスが著した『戦争と平和の法』では、国際法の必要性が主張されている。\n",
      "グロティウスが著した『戦争と平和の法』では、戦争の規制が唱えられている。\n",
      "ナショナリズムの高まりと徴兵制の導入によって、国民を戦争に総動員する時代が始まった。\n",
      "第一次世界大戦での戦争の長期化は、総力戦体制を確立した。\n",
      "\n",
      "original sentence count=293\n",
      "Shuca output: オランダは、鎖国時代の日本における西欧諸国唯一の貿易相手として長崎の出島に出入りした。\n",
      "17世紀、オランダ出身の法学者グロティウスは『戦争と平和の法』を著した。\n",
      "グロティウスの学説はその後の国際法の基礎となった。\n",
      "オレンジ自由国とトランスヴァール共和国は、南アフリカ戦争でイギリスによって征服された。\n",
      "オランダは、19世紀に蘭印で、コーヒーなどの商品作物の強制栽培制度を実施した。\n",
      "蘭印は太平洋戦争で日本に占領された。\n",
      "1992年のマーストリヒト条約によってヨーロッパ連合が発足した。\n",
      "オランダはハプスブルク家スペインによって統治されていた。\n",
      "17世紀、グロティウスは『海洋自由論』を著した。\n",
      "グロティウスは『海洋自由論』で、海洋貿易の自由を主張した。\n",
      "グロティウスは『海洋自由論』で、国際法の確立を促した。\n",
      "オランダは東南アジアにおいて、コーヒーなどの商品作物に対して強制栽培制度を実施した。\n",
      "19世紀末、オレンジ自由国は南アフリカ戦争の結果、イギリスに占領された。\n",
      "20世紀、太平洋戦争が勃発した。\n",
      "太平洋戦争中、オランダ領インドネシアは日本に占領された。\n",
      "太平洋戦争後、ハーグ協定によってオランダ領インドネシアは独立した。\n",
      "オランダは1851年の独立宣言によりハプスブルク家の支配から独立した。\n",
      "グロティウスは、17世紀のオランダの法学者である。\n",
      "グロティウスは、1625年に『戦争と平和の法』を著した。\n",
      "\n",
      "original sentence count=281\n",
      "Shuca output: ロシアはウィーン体制護持のため、ポーランドや、ハンガリーなどの反乱を鎮圧した。\n",
      "ロシアはクリミア戦争で敗北した。\n",
      "クリミア戦争後、パリ条約で黒海が中立化された。\n",
      "ロシアは、カージャール朝とトルコマンチャーイ条約を結んだ。\n",
      "ロシアはトルキスタンのイリ地方に進出した。\n",
      "イギリスは、インド防衛の観点からアフガニスタンを保護国化した。\n",
      "ロシアは条約により、沿海州を獲得した。\n",
      "ウィーン会議でロシアはポーランドを支配した。\n",
      "ロシアはクリミア戦争に介入した。\n",
      "ロシアはクリミア戦争で地中海地域への南下を試みた。\n",
      "北京条約で沿海州を獲得した。\n",
      "沿海州にウラジヴォストーク港を建設した。\n",
      "中国ではイリ地方に出兵した。\n",
      "中央アジアではカージャール朝イランとトルコマンチャーイ条約を締結した。\n",
      "ニコライ1世はトルコマンチャーイ条約でカージャール朝ペルシアからアルメニアを奪取し、治外法権を獲得した。\n",
      "ニコライ1世は1853～56年のクリミア戦争でトルコと戦い敗れた。\n",
      "アレクサンドル2世は1860年の北京条約でウスリー川以東の沿海州を領有した。\n",
      "ロシアは1881年のイリ条約で露清間のイリ地方における国境を画定した。\n",
      "ロシアは，カフカスでは１９世紀初頭にグルジアを直轄植民地とし，さらにアルメニアの帰属をめぐって，イランのカージャール朝と二度にわたって戦い，１８２８年のトルコマンチャーイ条約でアルメニアとアゼルバイジャン北半を植民地とした。\n",
      "\n",
      "original sentence count=45\n",
      "Shuca output: 古代エジプトは、ナイル川を中心に、古王国から新王国まで、長らく独立王朝が栄えた。\n",
      "クレオパトラは、ローマの内乱に際し、アクティウムの海戦に敗れた。\n",
      "アクティウムの海戦の後、エジプトはローマの属州となった。\n",
      "イスラム教勢力は紀元7世紀にアラビア半島を統一した。\n",
      "イスラム教勢力は東ローマ帝国とササン朝ペルシアの対立に乗じた。\n",
      "イスラム教勢力は版図を拡大した。\n",
      "イスラム教勢力はエジプトを征服した。\n",
      "ファーティマ朝では、1196年にサラディンが宰相となった。\n",
      "十字軍はイスラム教勢力から聖地エルサレムを奪還しようとした。\n",
      "サラディンは十字軍に対抗した。\n",
      "近代には、フランスのナポレオンがエジプトに進攻した。\n",
      "ナポレオンがエジプトに進攻したのは、イギリス本国とインドとの連絡線を断つためだった。\n",
      "ナポレオンは、エジプトで解放者を称した。\n",
      "ナポレオンは、エジプトで民衆の抵抗に遭った。\n",
      "対仏戦争に際して、オスマン帝国は、ムハンマド・アリーをエジプトに派遣した。\n",
      "ムハンマド・アリーは、エジプトに王朝を建てた。\n",
      "ムハンマド・アリー朝はイギリスによる内政への介入を受けた。\n",
      "ムハンマド・アリー朝はイギリスの保護国となった。\n",
      "エジプトはナイル川流域に穀倉地帯を形成した。\n",
      "7世紀以降、イスラム教徒はアフリカにまで勢力を拡大した。\n",
      "\n",
      "original sentence count=31\n",
      "Shuca output: アヘン戦争後、中国から、多くの移民が苦力として移住した。\n",
      "華僑は、マレー半島におけるイギリスの海峡植民地のサトウキビ・プランテーションで働いた。\n",
      "華僑は、利権回収運動を資金的に援助した。\n",
      "利権回収運動は、鉄道敷設権や鉱山採決権などの利権の回収を目指した。\n",
      "華僑は、孫文の革命運動を資金的に援助した。\n",
      "孫文の革命運動は清朝を打倒しようとした。\n",
      "明代以降、海禁政策が行われた。\n",
      "アヘン戦争での敗北で、海禁政策は廃止された。\n",
      "海禁政策廃止後、中国では海外渡航の自由化が承認された。\n",
      "アヘン戦争後、中国人労働者の生活は困窮していた。\n",
      "イギリスは海峡植民地を建設した。\n",
      "イギリスは海峡植民地を拠点に英領マラヤを建国した。\n",
      "イギリスは海峡植民地を中心に、ゴム・プランテーションを展開した。\n",
      "清は1840年のアヘン戦争の敗北により開国した。\n",
      "西部開拓の進展や南北戦争後にリンカーンが行った1863年の奴隷解放宣言、イギリスによる1833年の植民地奴隷制の廃止で、海峡植民地や南米のサトウキビ・プランテーションに於ける労働者の不足が深刻化した。\n",
      "\n",
      "original sentence count=41\n",
      "Shuca output: 30年戦争は、ウェストファリア条約によって終止符が打たれた。\n",
      "ウェストファリア条約は多くの勢力が参加して調印された。\n",
      "ウェストファリア条約によって、主権国家体制が成立した。\n",
      "グロティウスは、『戦争と平和の法』を著した。\n",
      "グロティウスは、『戦争と平和の法』で戦時において各国の守るべき権利・義務を述べた。\n",
      "諸外国に対して、徴兵制による国民軍が用いられた。\n",
      "第一次世界大戦では、ウィルソンによる十四カ条の平和原則が見られた。\n",
      "第一次世界大戦では、ソ連が平和に関する布告を発表した。\n",
      "第一次世界大戦後には、国際連盟が設立された。\n",
      "国際連盟は、集団安全保障の枠組みである。\n",
      "三十年戦争の講和条約はウェストファリア条約であった。\n",
      "ウェストファリア条約によってヨーロッパの主権国家体制が確立された。\n",
      "三十年戦争の惨禍を受けて、グロティウスは『戦争と平和の法』を著した。\n",
      "グロティウスが著した『戦争と平和の法』では、国際法の必要性が主張されている。\n",
      "グロティウスが著した『戦争と平和の法』では、戦争の規制が唱えられている。\n",
      "ナショナリズムの高まりと徴兵制の導入によって、国民を戦争に総動員する時代が始まった。\n",
      "第一次世界大戦での戦争の長期化は、総力戦体制を確立した。\n",
      "\n",
      "original sentence count=31\n",
      "Shuca output: オランダは、鎖国時代の日本における西欧諸国唯一の貿易相手として長崎の出島に出入りした。\n",
      "17世紀、オランダ出身の法学者グロティウスは『戦争と平和の法』を著した。\n",
      "グロティウスの学説はその後の国際法の基礎となった。\n",
      "オレンジ自由国とトランスヴァール共和国は、南アフリカ戦争でイギリスによって征服された。\n",
      "オランダは、19世紀に蘭印で、コーヒーなどの商品作物の強制栽培制度を実施した。\n",
      "蘭印は太平洋戦争で日本に占領された。\n",
      "1992年のマーストリヒト条約によってヨーロッパ連合が発足した。\n",
      "オランダはハプスブルク家スペインによって統治されていた。\n",
      "17世紀、グロティウスは『海洋自由論』を著した。\n",
      "グロティウスは『海洋自由論』で、海洋貿易の自由を主張した。\n",
      "グロティウスは『海洋自由論』で、国際法の確立を促した。\n",
      "オランダは東南アジアにおいて、コーヒーなどの商品作物に対して強制栽培制度を実施した。\n",
      "19世紀末、オレンジ自由国は南アフリカ戦争の結果、イギリスに占領された。\n",
      "20世紀、太平洋戦争が勃発した。\n",
      "太平洋戦争中、オランダ領インドネシアは日本に占領された。\n",
      "太平洋戦争後、ハーグ協定によってオランダ領インドネシアは独立した。\n",
      "オランダは1851年の独立宣言によりハプスブルク家の支配から独立した。\n",
      "グロティウスは、17世紀のオランダの法学者である。\n",
      "グロティウスは、1625年に『戦争と平和の法』を著した。\n",
      "\n",
      "original sentence count=24\n",
      "Shuca output: ロシアはウィーン体制護持のため、ポーランドや、ハンガリーなどの反乱を鎮圧した。\n",
      "ロシアはクリミア戦争で敗北した。\n",
      "クリミア戦争後、パリ条約で黒海が中立化された。\n",
      "ロシアは、カージャール朝とトルコマンチャーイ条約を結んだ。\n",
      "ロシアはトルキスタンのイリ地方に進出した。\n",
      "イギリスは、インド防衛の観点からアフガニスタンを保護国化した。\n",
      "ロシアは条約により、沿海州を獲得した。\n",
      "ウィーン会議でロシアはポーランドを支配した。\n",
      "ロシアはクリミア戦争に介入した。\n",
      "ロシアはクリミア戦争で地中海地域への南下を試みた。\n",
      "北京条約で沿海州を獲得した。\n",
      "沿海州にウラジヴォストーク港を建設した。\n",
      "中国ではイリ地方に出兵した。\n",
      "中央アジアではカージャール朝イランとトルコマンチャーイ条約を締結した。\n",
      "イギリスはアフガニスタンを保護国化した。\n",
      "1814～15年のウィーン会議でロシア皇帝はポーランド王国の王位を兼ねることになった。\n",
      "ニコライ1世はトルコマンチャーイ条約でカージャール朝ペルシアからアルメニアを奪取し、治外法権を獲得した。\n",
      "ニコライ1世は1853～56年のクリミア戦争でトルコと戦い敗れた。\n",
      "トルコは1853～56年のクリミア戦争でイギリス・フランス・オーストリア・サルデーニャの支援を受けた。\n",
      "アレクサンドル2世は1860年の北京条約でウスリー川以東の沿海州を領有した。\n",
      "ロシアは1881年のイリ条約で露清間のイリ地方における国境を画定した。\n"
     ]
    }
   ],
   "source": [
    "temp_run_folder_path = 'qalab3-essay-phase2/'\n",
    "ref_folder_path_ja = 'qalab3-essay-phase2/_references/qalab3-ja-essay-phase2'\n",
    "\n",
    "ans_file_name_ja = 'qalab3-ja-phase2-answersheet-essay' \n",
    "ans_tree_ja = et.parse('{}/{}.xml'.format(ref_folder_path_ja, ans_file_name_ja))\n",
    "ans_root_ja = ans_tree_ja.getroot()\n",
    "\n",
    "passage_file_id_list = ['ExP10_revised', 'GSN+ExP10_revised', 'GSN']\n",
    "for passage_file_id in passage_file_id_list:\n",
    "    passage_file_name_ja = 'qalab3-ja-phase2-essay-extraction-{}.xml'.format(passage_file_id)\n",
    "    exp_tree_ja = et.parse('{}/{}'.format(ref_folder_path_ja, passage_file_name_ja))\n",
    "    exp_root_ja = exp_tree_ja.getroot()\n",
    "\n",
    "    summary_set_ja = get_summary_set_for_all(exp_root_ja, ans_root_ja, summarize_func_shuca_ja, passage_filter_ja)\n",
    "    write_xml_by_extracton_file(summary_set_ja, ans_tree_ja, temp_run_folder_path, ans_file_name_ja, passage_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "def sumy_lex_rank_en(text, length):\n",
    "    LANGUAGE = 'english'\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "    summary_tokens = []\n",
    "    current_length = 0\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        passage_tokens = str(sentence).split(' ')\n",
    "        if not summary_tokens:\n",
    "            summary_tokens.extend(passage_tokens)\n",
    "            current_length += len(passage_tokens)\n",
    "            if len(summary_tokens) > length:\n",
    "                return ' '.join(summary_tokens[:length])\n",
    "        elif current_length + len(passage_tokens) <= length:\n",
    "            summary_tokens.extend(passage_tokens)\n",
    "            current_length += len(passage_tokens)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(summary_tokens[:length])\n",
    "\n",
    "\n",
    "def summarize_func_sumy_lex_rank_en(passage_and_src_pair_list, len_limit):\n",
    "    print('\\noriginal sentence count={}'.format(len(passage_and_src_pair_list)))\n",
    "    passage_and_src_pair_list = passage_and_src_pair_list[:30]\n",
    "    passages, src_list = zip(*passage_and_src_pair_list)\n",
    "    passage_text = '\\n'.join(passages)\n",
    "    summary_text = sumy_lex_rank_en(passage_text, len_limit)\n",
    "    return summary_text, src_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original sentence count=107\n",
      "\n",
      "original sentence count=34\n",
      "\n",
      "original sentence count=54\n",
      "\n",
      "original sentence count=74\n",
      "\n",
      "original sentence count=78\n",
      "\n",
      "original sentence count=151\n",
      "\n",
      "original sentence count=57\n",
      "\n",
      "original sentence count=86\n",
      "\n",
      "original sentence count=104\n",
      "\n",
      "original sentence count=105\n",
      "\n",
      "original sentence count=44\n",
      "\n",
      "original sentence count=23\n",
      "\n",
      "original sentence count=32\n",
      "\n",
      "original sentence count=30\n",
      "\n",
      "original sentence count=27\n"
     ]
    }
   ],
   "source": [
    "temp_run_folder_path = 'qalab3-essay-phase2/'\n",
    "ref_folder_path_en = 'qalab3-essay-phase2/_references/qalab3-en-essay-phase2'\n",
    "\n",
    "ans_file_name_en = 'qalab3-en-phase2-answersheet-essay' \n",
    "ans_tree_en = et.parse('{}/{}.xml'.format(ref_folder_path_en, ans_file_name_en))\n",
    "ans_root_en = ans_tree_en.getroot()\n",
    "\n",
    "passage_file_id_list = ['ExP10_revised', 'GSN+ExP10_revised', 'GSN']\n",
    "for passage_file_id in passage_file_id_list:\n",
    "    passage_file_name_en = 'qalab3-en-phase2-essay-extraction-{}.xml'.format(passage_file_id)\n",
    "    exp_tree_en = et.parse('{}/{}'.format(ref_folder_path_en, passage_file_name_en))\n",
    "    exp_root_en = exp_tree_en.getroot()\n",
    "\n",
    "    summary_set_en = get_summary_set_for_all(exp_root_en, ans_root_en, summarize_func_sumy_lex_rank_en, passage_filter_en)\n",
    "    write_xml_by_extracton_file(summary_set_en, ans_tree_en, temp_run_folder_path, ans_file_name_en, passage_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_passage_list_for_all(exp_root, ans_root, passage_filter_func):\n",
    "    passage_list = []\n",
    "    for topic in exp_root.iter('TOPIC'):\n",
    "        topic_id = topic.get('ID')\n",
    "        ans_section = ans_root.find('answer_section/[@id=\"{}\"]'.format(topic_id))\n",
    "        ans_len_limit = get_length_limit(ans_section)\n",
    "        keywords = get_element_texts(ans_section, 'keyword_set/keyword')\n",
    "        passage_and_src_pairs = []\n",
    "        for passage_set in topic.iter('PASSAGE_SET'):\n",
    "            passage_set_name = passage_set.get('FILE_NAME')\n",
    "\n",
    "            for passage in passage_set.iter('PASSAGE'):\n",
    "                passage_id = passage_set_name + '_' + passage.get('SOURCE_ID')\n",
    "                passage_text = ''.join(passage.itertext()).strip()\n",
    "                passage_and_src_pairs.append((passage_text, passage_id))\n",
    "        passage_and_src_pairs = passage_filter_func(passage_and_src_pairs, keywords)\n",
    "        passage_list.append((topic_id, passage_and_src_pairs))\n",
    "    return passage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from tensorflow.core.example import example_pb2\n",
    "\n",
    "SUMMARY_FIELD_NAME = 'abstract'\n",
    "TEXT_FIELD_NAME = 'article'\n",
    "\n",
    "def get_pb2_str(text, summary):\n",
    "    tf_example = example_pb2.Example()\n",
    "    tf_example.features.feature[TEXT_FIELD_NAME].bytes_list.value.extend([text.encode('utf-8')])\n",
    "    tf_example.features.feature[SUMMARY_FIELD_NAME].bytes_list.value.extend([summary.encode('utf-8')])\n",
    "    return tf_example.SerializeToString()\n",
    "\n",
    "\n",
    "def pack_tensorflow_pb2_str(tensorflow_pb2_str):\n",
    "    str_len = len(tensorflow_pb2_str)\n",
    "    return struct.pack('q{}s'.format(str_len), str_len, tensorflow_pb2_str)\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    with Popen(['java', 'edu.stanford.nlp.process.PTBTokenizer', '-preserveLines'],\n",
    "               stdin=PIPE, stdout=PIPE, stderr=PIPE) as tokenizer_proc:\n",
    "        out, err = tokenizer_proc.communicate(input=text.encode('UTF-8'))\n",
    "        return out.decode('UTF-8')\n",
    "\n",
    "\n",
    "import MeCab\n",
    "\n",
    "\n",
    "def tokenize_ja(text):\n",
    "    mecab = MeCab.Tagger('-Owakati')\n",
    "    return mecab.parse(text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "\n",
    "\n",
    "temp_run_folder_path = '../'\n",
    "ref_folder_path_ja = '../_references/qalab3-ja-essay-phase2'\n",
    "\n",
    "ans_file_name_ja = 'qalab3-ja-phase2-answersheet-essay' \n",
    "ans_tree_ja = et.parse('{}/{}.xml'.format(ref_folder_path_ja, ans_file_name_ja))\n",
    "ans_root_ja = ans_tree_ja.getroot()\n",
    "\n",
    "old_vocab = '../../summarization_corpus_to_tensorflow_input/finished_files/vocab-livedoor_jawikinews.txt'\n",
    "old_vocab_dict = {}\n",
    "with open(old_vocab) as ov:\n",
    "    reader = csv.reader(ov, delimiter=' ', quoting=3)\n",
    "    old_vocab_dict = {rows[0]: int(rows[1]) for rows in reader}\n",
    "vocab_counter = collections.Counter(old_vocab_dict)\n",
    "passage_file_id_list = ['ExP10_revised', 'GSN+ExP10_revised', 'GSN']\n",
    "for passage_file_id in passage_file_id_list:\n",
    "    passage_file_name_ja = 'qalab3-ja-phase2-essay-extraction-{}.xml'.format(passage_file_id)\n",
    "    exp_tree_ja = et.parse('{}/{}'.format(ref_folder_path_ja, passage_file_name_ja))\n",
    "    exp_root_ja = exp_tree_ja.getroot()\n",
    "\n",
    "    passage_list_ja = get_passage_list_for_all(exp_root_ja, ans_root_ja, passage_filter_ja)\n",
    "    for topic_id, passage_and_src_pairs in passage_list_ja:\n",
    "        tokenized_sentences =[tokenize_ja(passage_src_pair[0]) for passage_src_pair in passage_and_src_pairs]\n",
    "        vocab_counter.update([str(t) for s in tokenized_sentences for t in str(s).split(' ')])\n",
    "        with open('test-{}-{}-ja.bin'.format(passage_file_id, topic_id), 'wb') as f:\n",
    "            for i in range(0, len(tokenized_sentences), 3):\n",
    "                passage_text = ' '.join(tokenized_sentences[i:i+3])\n",
    "                f.write(pack_tensorflow_pb2_str(get_pb2_str(passage_text, 'dummy_summary')))\n",
    "\n",
    "with open('vocab-qalab.txt', 'w') as v:\n",
    "    for word, count in vocab_counter.most_common(50000):\n",
    "        v.write(word + ' ' + str(count) + '\\n')\n",
    "    # write_xml_by_extracton_file(summary_set_ja, ans_tree_ja, temp_run_folder_path, ans_file_name_ja, passage_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "\n",
    "\n",
    "temp_run_folder_path = '../'\n",
    "ref_folder_path_en = '../_references/qalab3-en-essay-phase2'\n",
    "\n",
    "ans_file_name_en = 'qalab3-en-phase2-answersheet-essay' \n",
    "ans_tree_en = et.parse('{}/{}.xml'.format(ref_folder_path_en, ans_file_name_en))\n",
    "ans_root_en = ans_tree_en.getroot()\n",
    "\n",
    "old_vocab = 'vocab-en.txt'\n",
    "old_vocab_dict = {}\n",
    "with open(old_vocab) as ov:\n",
    "    reader = csv.reader(ov, delimiter=' ', quoting=3)\n",
    "    old_vocab_dict = {rows[0]: int(rows[1]) for rows in reader}\n",
    "vocab_counter = collections.Counter(old_vocab_dict)\n",
    "passage_file_id_list = ['ExP10_revised', 'GSN+ExP10_revised', 'GSN']\n",
    "for passage_file_id in passage_file_id_list:\n",
    "    passage_file_name_en = 'qalab3-en-phase2-essay-extraction-{}.xml'.format(passage_file_id)\n",
    "    exp_tree_en = et.parse('{}/{}'.format(ref_folder_path_en, passage_file_name_en))\n",
    "    exp_root_en = exp_tree_en.getroot()\n",
    "\n",
    "    passage_list_en = get_passage_list_for_all(exp_root_en, ans_root_en, passage_filter_en)\n",
    "    for topic_id, passage_and_src_pairs in passage_list_en:\n",
    "        tokenized_sentences =[tokenize_en(passage_src_pair[0]) for passage_src_pair in passage_and_src_pairs]\n",
    "        vocab_counter.update([str(t) for s in tokenized_sentences for t in str(s).split(' ')])\n",
    "        with open('test-{}-{}-en.bin'.format(passage_file_id, topic_id), 'wb') as f:\n",
    "            for i in range(0, len(tokenized_sentences), 3):\n",
    "                passage_text = ' '.join(tokenized_sentences[i:i+3])\n",
    "                f.write(pack_tensorflow_pb2_str(get_pb2_str(passage_text, 'dummy_summary')))\n",
    "\n",
    "with open('vocab-en-qalab.txt', 'w') as v:\n",
    "    for word, count in vocab_counter.most_common(50000):\n",
    "        v.write(word + ' ' + str(count) + '\\n')\n",
    "    # write_xml_by_extracton_file(summary_set_ja, ans_tree_ja, temp_run_folder_path, ans_file_name_ja, passage_file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
